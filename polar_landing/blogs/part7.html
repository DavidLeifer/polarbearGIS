<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
	<title></title>
	<meta name="generator" content="LibreOffice 7.0.3.1 (MacOSX)"/>
	<meta name="created" content="2021-01-09T11:17:00.787621792"/>
	<meta name="changed" content="2021-06-27T15:59:37.935769133"/>
	<style type="text/css">
		@page { size: 8.5in 11in; margin: 0.79in }
		p { margin-bottom: 0.1in; line-height: 115%; background: transparent }
		h3 { margin-top: 0.1in; margin-bottom: 0.08in; background: transparent; page-break-after: avoid }
		h3.western { font-family: "Liberation Serif", serif; font-size: 14pt; font-weight: bold }
		h3.cjk { font-family: "Songti SC"; font-size: 14pt; font-weight: bold }
		h3.ctl { font-family: "Arial Unicode MS"; font-size: 14pt; font-weight: bold }
		pre { font-family: "Liberation Mono", monospace; font-size: 10pt; background: transparent }
		strong { font-weight: bold }
		a:link { color: #000080; so-language: zxx; text-decoration: underline }
		a:visited { color: #800000; so-language: zxx; text-decoration: underline }
	</style>
</head>
<body lang="en-US" link="#000080" vlink="#800000" dir="ltr">
<hr/>

<h3 class="western">Part 7: Pearson Correlation Between Yearly
January Precipitation and El Nino 3.4 Index (Dave goes 4D&nbsp;Python)</h3>
<p><br/>
<br/>

</p>
<p><strong>Plug</strong> to <a href="http://nhuthehoang.com/" target="_blank">http://nhuthehoang.com/</a>
for telling me about GCP and Geoserver.</p>
<p>This series is me chronicling my adventure making a data pipeline.
This pipe is outlined on GitHub here in the readme:
<a href="https://gitlab.com/davleifer/polarbearGIS">https://gitlab.com/davleifer/polarbearGIS</a></p>
<p><strong>Part </strong><strong>1</strong><strong>:
</strong><strong><a href="https://www.davidjleifer.com/blogs/part1.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part</span></u></span></font></a></strong><strong><a href="https://www.davidjleifer.com/blogs/part1.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">1</span></u></span></font></a></strong><strong><a href="https://www.davidjleifer.com/blogs/part1.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">.html</span></u></span></font></a></strong></p>
<p><strong>Part 2: </strong><strong><a href="https://www.davidjleifer.com/blogs/part2.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part2.html</span></u></span></font></a></strong></p>
<p><strong>Part 3:</strong>
<strong><a href="https://www.davidjleifer.com/blogs/part3.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part3.html</span></u></span></font></a></strong></p>
<p><strong>Part 4: </strong><strong><a href="https://www.davidjleifer.com/blogs/part4.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part4.html</span></u></span></font></a></strong></p>
<p><strong>Part 5: </strong><strong><a href="https://www.davidjleifer.com/blogs/part5.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part5.html</span></u></span></font></a></strong></p>
<p><strong>Part 6: </strong><a href="https://www.davidjleifer.com/blogs/part6.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/part6.html</u></span></font></a></p>
<p><strong>Part </strong><strong>8</strong><strong>:
</strong><a href="https://www.davidjleifer.com/blogs/part8.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/part8.html</u></span></font></a></p>
<p><strong>Part </strong><strong>9</strong><strong>:
</strong><a href="https://www.davidjleifer.com/blogs/part9.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/part9.html</u></span></font></a></p>
<p><strong>Part </strong><strong>1</strong><strong>0</strong><strong>:
</strong><a href="https://www.davidjleifer.com/blogs/part10.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/par</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part10.html"><font color="#000080"><span lang="zxx"><u>t1</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part10.html"><font color="#000080"><span lang="zxx"><u>0</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part10.html"><font color="#000080"><span lang="zxx"><u>.html</u></span></font></a></p>
<p><strong>Part </strong><strong>1</strong><strong>1</strong><strong>:
</strong><a href="https://www.davidjleifer.com/blogs/part11.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/par</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part11.html"><font color="#000080"><span lang="zxx"><u>t1</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part11.html"><font color="#000080"><span lang="zxx"><u>1</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part11.html"><font color="#000080"><span lang="zxx"><u>.html</u></span></font></a></p>
<p><b>Part 12: </b><a href="https://www.davidjleifer.com/blogs/part12.html"><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part12.html</span></a></p>
<p><b>Part 13: </b><a href="https://www.davidjleifer.com/blogs/part13.html"><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part13.html</span></a></p>
<p><br/>
<br/>

</p>
<p>Today we will be using Python to see how well the El Nino 3.4
Index correlates with yearly precipitation in January across the
lower 48 continental United States of America. If you don’t know
what El Nino 3.4 is, I encourage you to read this from NOAA:&nbsp;</p>
<p><a href="https://climatedataguide.ucar.edu/climate-data/nino-sst-indices-nino-12-3-34-4-oni-and-tni" target="_blank">https://climatedataguide.ucar.edu/climate-data/nino-sst-indices-nino-12-3-34-4-oni-and-tni</a></p>
<p>If you are still not satisfied, read more here:
<a href="https://www.ncdc.noaa.gov/teleconnections/enso/indicators/sst/" target="_blank">https://www.ncdc.noaa.gov/teleconnections/enso/indicators/sst/</a></p>
<p>It’s basically an index of sea surface temperature along the
equator in the Pacific ocean. It is said to control precipitation and
temperature in Southeast and Pacific North West in what is known as
the El Nino- Southern Oscillation (ENSO). Read more here:
<a href="https://oceanservice.noaa.gov/facts/ninonina.html" target="_blank">https://oceanservice.noaa.gov/facts/ninonina.html</a></p>
<p>The important bit from those links is this: “The Niño 3.4 index
typically uses a 5-month running mean, and El Niño or La Niña
events are defined when the Niño 3.4 SSTs exceed +/- 0.4C for a
period of six months or more.”</p>
<p>I mean the implications are pretty obvious, if you can predict how
much precipitation falls in the growing season, you can estimate how
much corn or soybeans will be yielded for the year.</p>
<p>We will be using three python scripts to do this. The first
prepares the files for input, the second runs Pearson Correlation on
the input, and the third is our XYZ tile converter.</p>
<p><strong>Part 1: Preparing the Files for Input into Pearson
Correlation Script</strong></p>
<p>Check out the python libraries we will be using to prepare our
raster files for correlation.</p>
<pre>import rasterio
import fiona
from shapely.geometry import shape
import rasterio.mask
import rasterio.features
import geopandas as gp
import numpy as np
import pandas as pd
import pyproj
import seaborn as sns
import scipy.stats as stats
from glob import glob
from osgeo import ogr, gdal, osr
from osgeo.gdalnumeric import *
from osgeo.gdalconst import *
from rasterio.warp import calculate_default_transform, reproject, Resampling
import os
import sys</pre><p>
Now we will get the current working directory of our project and read
in our nino3.4.xlsx index. We create a list of all the&nbsp;.bil
files in the data ppt section. We also read in an example tif with
GDAL that will be used in our for loop to transform an array in an
output. We finally read in the same shapefile with fiona to clip an
array in our for loop.</p>
<pre>#get current working dir
cwd = os.getcwd()
#read in index
df = pd.read_excel(cwd + '/data/nino34.xlsx', sheet_name='Sheet1')
ppt_data_dir = cwd + '/data/ppt/'
ppt_file_list = glob(os.path.join(ppt_data_dir, '*.bil'))
example_tif = cwd + &quot;/data/ppt_bil2tif_resize_1981.tif&quot;
#used to set gdal transform way down in the for loop
data0 = gdal.Open(example_tif)
vectorize_output = cwd + &quot;/data/timeseries_19910101_shp_mask/timeseries_19910101_shp_mask.shp&quot;
with fiona.open(vectorize_output, &quot;r&quot;) as shapefile:
    vectorize_output_shp = [feature[&quot;geometry&quot;] for feature in shapefile]</pre><p>
Now we loop over each&nbsp;.bil file in the data/ppt/ directory. The
first part of this for loop strips the directory string to get the
year. Also, we set the width and height for our rasters.</p>
<pre>for file in ppt_file_list:
    base = os.path.basename(file)
    year = base[23:27]
    width = 1405
    height = 621</pre><p>
The next thing we do is save each&nbsp;.bil as a&nbsp;.tif. This
output directory will be used in the actual Pearson Correlation
script in Part 2 of this post. Keep in mind this is in the for loop
still so we are doing this for all 33 years.</p>
<pre>with rasterio.open(file) as src:
        thumbnail = src.read(1, out_shape=(1, int(height), int(width)))
#<a href="https://rasterio.readthedocs.io/en/latest/topics/writing.html" target="_blank">https://rasterio.readthedocs.io/en/latest/topics/writing.html</a>
        #save resized bil to tif
        tif = &quot;.tif&quot;
        rsz_bil = cwd + '/data/ppt_bil2tif_resize/ppt_bil2tif_resize_'
        output_bil_path2tif = rsz_bil + year + tif
        with rasterio.Env():
            profile = src.profile
            profile.update(
                dtype=rasterio.int32,
                count=1,
                compress='lzw',
                height=621,
                width=1405,
                crs=4269,)
            with rasterio.open(output_bil_path2tif, 'w', **profile,) as dst:
                dst.write(thumbnail.astype(rasterio.int32), 1)
                print(dst.shape)
                print(dst.crs)</pre><p>
Then we create a 1405 by 621 numpy array with the z value being the
nino34 dataframe. We write that to disk as a tif with GDAL using the
numpy array and an example tif for the transformation AKA variable
named data0 (as specified previously).</p>
<pre>#create a np mask of the year's nino34.xlsx index and save as tif
    df.loc[df['Year'] == int(year), 'Index']
    yer_index = df.loc[df['Year'] == int(year), 'Index'].iloc[0]
    np_mask = (621,1405)
    yi_array = np.full(np_mask,yer_index)
    #<a href="https://gis.stackexchange.com/questions/37238/writing-numpy-array-to-raster-file" target="_blank">https://gis.stackexchange.com/questions/37238/writing-numpy-array-to-raster-file</a>
    #write array with gdal
    cor_mask_dst_filename = cwd + '/data/ppt_cor_mask/ppt_cor_mask_'
    tif = &quot;.tif&quot;
dst_filename = cor_mask_dst_filename + year + tif
    x_pixels = 1405  # number of pixels in x
    y_pixels = 621  # number of pixels in y
    driver = gdal.GetDriverByName('GTiff')
    dataset = driver.Create(dst_filename,x_pixels, y_pixels, 1,gdal.GDT_Float32)
    dataset.GetRasterBand(1).WriteArray(yi_array)
    # following code is adding GeoTranform and Projection
    geotrans=data0.GetGeoTransform()  #get GeoTranform from existed 'data0'
    #proj=data0.GetProjection() #you can get from a exsited tif or import 
    dataset.SetGeoTransform(geotrans)
    srs = osr.SpatialReference()
    srs.SetWellKnownGeogCS('NAD83')
    dataset.SetProjection(srs.ExportToWkt())
    dataset.FlushCache()
    band=dataset.GetRasterBand(1)
    band.SetNoDataValue(-9999)
    band=None
    dataset=None</pre><p>
Now we need to clip the index raster with the fiona shapefile of the
lower 48.&nbsp;</p>
<pre>#read in dst_filename AKA a raster mask from index and clip it with the vectorize_output_shp from qgis
    with rasterio.open(dst_filename) as src:
        out_image, out_transform = rasterio.mask.mask(src, vectorize_output_shp, crop=True)
        out_meta = src.meta
    out_meta.update({&quot;driver&quot;: &quot;GTiff&quot;,
                     &quot;height&quot;: 621,
                     &quot;width&quot;: 1405,
                     &quot;transform&quot;: out_transform,
                     &quot;crs&quot;: &quot;NAD83&quot;})
    #save and clip raster index mask as file
    pearson_staging = cwd + &quot;/data/ppt_pearson_output/ppt_pearson_output_&quot;
    pearson_output_path = pearson_staging + year + tif
    with rasterio.open(pearson_output_path, &quot;w&quot;, **out_meta) as dest:
        dest.write(out_image)
    #load clip raster index as a file with rasterio
    #with rasterio.open(pearson_output_path) as src:
    #    pearson_output_path_rasterio = src.read(1, out_shape=(1, int(src.height), int(src.width)))</pre><p>
Once every year is done we will have a directory containing 33 tifs
of precipitation data for the US and 33 tifs of el nino index shaped
and spatially organized into the shape of the US (each year is a
different index value). We will be comparing these two rasters with a
moving correlation window in Part 2.</p>
<p><strong>Part 2: Moving Window Pearson Correlation Between Yearly
El Nino 3.4 Index and Yearly January Precipitation Rasters</strong></p>
<p>For this script, we will import a bunch of libs and set two
important lists from our two tif directories in Part 1: One for
nino34 and one for precipitation data for that year’s January. Also
sort both lists into order (otherwise it would be out of order and
the years would not match up).</p>
<pre>import rasterio
import pandas as pd
import numpy as np
from scipy.stats.stats import pearsonr
import shutil
from rasterio.plot import show
from osgeo import ogr, gdal, osr
from osgeo.gdalnumeric import *
from osgeo.gdalconst import *
import os
from glob import glob
#avoid annoying PROJ_LIB error
os.environ[&quot;PROJ_LIB&quot;]=&quot;/Applications/QGIS.app/Contents/Resources/proj&quot;
#get current working dir
cwd = os.getcwd()
#read in ppt_bil2tif_resize as list of file directories
ppt_bil2tif_resize = cwd + '/data/ppt_bil2tif_resize/'
ppt_bil2tif_resize_list = glob(os.path.join(ppt_bil2tif_resize, '*.tif'))
ppt_bil2tif_resize_list.sort()
#print(ppt_bil2tif_resize_list)
#read in index files as list of file directories
ppt_pearson_output = cwd + '/data/ppt_pearson_output/'
ppt_pearson_output_list = glob(os.path.join(ppt_pearson_output, '*.tif'))
ppt_pearson_output_list.sort()
#print(ppt_pearson_output_list)</pre><p>
Then we loop over the pair of those directories, get the year from
the path, and put the two paired paths into their own rasterio array.</p>
<pre>for example_tif, index_tif in zip(ppt_bil2tif_resize_list,ppt_pearson_output_list):
    base = os.path.basename(example_tif)
    year = base[19:23]
    base2 = os.path.basename(index_tif)
    #load in precipitation raster tif
    width = 1405
    height = 621
    with rasterio.open(example_tif, mode=&quot;r+&quot;) as src:
        ppt_raster = src.read(1, out_shape=(1, int(height), int(width)))
    #load in 1981 index mask raster tif
    width = 1405
    height = 621
    with rasterio.open(index_tif) as src:
        index_raster = src.read(1, out_shape=(1, int(height), int(width)))</pre><p>
We then create an empty list, where we will append the output lists
for each row from pandas rolling correlation into this list, making a
list of lists. Then for each paired index array and precipitation
array (we are still in the for loop here), we then loop over that
pair of rasterio arrays. Then create two more empty lists to hold our
unfurled rasterio array. We then create another for loop and append
each value to the two empty lists. The corr1 variable represents our
precipitation raster and corr2 represents our index raster. Should be
two lists with 1405 in each list. And we do this 621 times. Then
convert each list into pandas dataframe and run the rolling
correlation window (3 rows by each dataframe, 6 total rows go into
each calculation). The first two values will be nans, but thats ok,
in our rasterio array we have nans representing the array around the
US but not actually within her borders. The important part is that it
outputs a single pandas dataframe of the same 1405 values. The
dataframe gets converted to a list, then appended to the empty list
created before (rolling_corr1_lst_stkd) creating a list of lists
(1405X621).</p>
<pre>#loop over both rasterios and conduct corr on moving 3x2 window each #cell, append to list to make list of lists
    rolling_corr1_lst_stkd = []
    for x, i in zip(ppt_raster, index_raster): 
        #create lists to hold precipitation raster values
        corr1 = []
        corr2 = []
        for xx, ii in zip(x, i):
            corr1.append(xx)
            corr2.append(ii)
        #list to panda dataframe
        df1 = pd.DataFrame(corr1)
        df2 = pd.DataFrame(corr2)
        #rolling correlation pandas (3Row by 2 Columns)
        rolling_corr1 = df1.rolling(3).corr(df2)
        #create list from pandas dataframe from rolling correlation
        rolling_corr1_lst = rolling_corr1.values.tolist()
        rolling_corr1_lst_stkd.append(rolling_corr1_lst)
    #unravel list of lists: rolling_corr1_lst_stkd
    unraveling = []
    for i in rolling_corr1_lst_stkd:
        for idx,ii in enumerate(i):
            unraveling.append(ii)
            #print(idx)
            #print(ii)</pre><p>
Then we unravel the list of lists with a for loop and another nested
for loop and append it to another list and convert that to numpy
array and scale the small correlation values in middle America by
10,000,000 (there appears to be erroneous numbers on edge cases
around the border). Then reshape into 621X1405 numpy array.</p>
<pre>#unravel list of lists: rolling_corr1_lst_stkd
    unraveling = []
    for i in rolling_corr1_lst_stkd:
        for idx,ii in enumerate(i):
            unraveling.append(ii)
            #print(idx)
            #print(ii)
    npa = np.asarray(unraveling, dtype=np.float32)
    #array scaled by 100,000,000,000
    scaled_array = np.multiply(npa, 10000000000)
    arr = scaled_array.astype('float32') 
    #reshape
    newarr = arr.reshape(621, 1405)</pre><p>
Figuring out nodata values are always fun…</p>
<pre>#set nan
    newarr[newarr == 0] = 'nan'
    newarr2 = np.nan_to_num(x=newarr,nan=-9999,posinf=.00001,neginf=-.00001)
    #print(newarr2)</pre><p>
Then save it and give it spatial and transform information from a
previously defined example tif of the US.</p>
<pre>#reshaped_flat_list_nan2num = np.nan_to_num(reshaped_flat_list)
    with rasterio.open(example_tif) as src:
        naip_data = src.read()
        naip_meta = src.profile
    # make any necessary changes to raster properties, e.g.:
    naip_meta['dtype'] = &quot;float32&quot;
    naip_meta['nodata'] = -9999
    data_dir = &quot;/data/ppt_pearson_final/ppt_pearson_final_&quot;
    dst_filename = cwd + data_dir + year + &quot;.tif&quot;
with rasterio.open(dst_filename, 'w', **naip_meta) as dst:
        dst.write(newarr2, 1)</pre><p>
After all that, exit the for loop and print a finished message.</p>
<pre style="margin-bottom: 0.2in">print(&quot;Finished ppt_cor_actually.py&quot;)</pre><p>
<strong>Part 3: Turning Correlation Raster Tiffs into XYZ Tiles For
Later Consumption</strong></p>
<p>Use this script here to turn the directory of Tiffs into XYZ tile
files.
<a href="https://davleifer.medium.com/part-5-how-to-use-python-for-qgis3-gdal-to-a-turn-a-directory-of-tifs-into-xyz-tiles-312d6c565f8" target="_blank">https://davleifer.medium.com/part-5-how-to-use-python-for-qgis3-gdal-to-a-turn-a-directory-of-tifs-into-xyz-tiles-312d6c565f8</a>&nbsp;</p>
<p>I changed the zoom range to be regional (so you cant zoom in super
close, thus saving time building tile files). Also changed raster
value ranges to be between -1 and 1 to show variation over middle
America (otherwise the values were all grouped together as the
same).&nbsp;</p>
<p>Heres a download of what it should look like:
<a href="https://drive.google.com/file/d/1ZIhsD2gGwP-i7eKD8hnOX-BepofbCRiy/view?usp=sharing" target="_blank">https://drive.google.com/file/d/1ZIhsD2gGwP-i7eKD8hnOX-BepofbCRiy/view?usp=sharing</a></p>
<p>Next up we will be displaying with Ol, npm, parcel, and jQuery.</p>
<hr/>

<p>This series is me chronicling my adventure making a data pipeline.
This pipe is outlined on GitHub here in the readme:
<a href="https://gitlab.com/davleifer/polarbearGIS">https://gitlab.com/davleifer/polarbearGIS</a></p>
<p><strong>Part </strong><strong>1</strong><strong>:
</strong><strong><a href="https://www.davidjleifer.com/blogs/part1.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part</span></u></span></font></a></strong><strong><a href="https://www.davidjleifer.com/blogs/part1.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">1</span></u></span></font></a></strong><strong><a href="https://www.davidjleifer.com/blogs/part1.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">.html</span></u></span></font></a></strong></p>
<p><strong>Part 2: </strong><strong><a href="https://www.davidjleifer.com/blogs/part2.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part2.html</span></u></span></font></a></strong></p>
<p><strong>Part 3:</strong>
<strong><a href="https://www.davidjleifer.com/blogs/part3.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part3.html</span></u></span></font></a></strong></p>
<p><strong>Part 4: </strong><strong><a href="https://www.davidjleifer.com/blogs/part4.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part4.html</span></u></span></font></a></strong></p>
<p><strong>Part 5: </strong><strong><a href="https://www.davidjleifer.com/blogs/part5.html"><font color="#000080"><span lang="zxx"><u><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part5.html</span></u></span></font></a></strong></p>
<p><strong>Part 6: </strong><a href="https://www.davidjleifer.com/blogs/part6.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/part6.html</u></span></font></a></p>
<p><strong>Part </strong><strong>8</strong><strong>:
</strong><a href="https://www.davidjleifer.com/blogs/part8.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/part8.html</u></span></font></a></p>
<p><strong>Part </strong><strong>9</strong><strong>:
</strong><a href="https://www.davidjleifer.com/blogs/part9.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/part9.html</u></span></font></a></p>
<p><strong>Part </strong><strong>1</strong><strong>0</strong><strong>:
</strong><a href="https://www.davidjleifer.com/blogs/part10.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/par</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part10.html"><font color="#000080"><span lang="zxx"><u>t1</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part10.html"><font color="#000080"><span lang="zxx"><u>0</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part10.html"><font color="#000080"><span lang="zxx"><u>.html</u></span></font></a></p>
<p><strong>Part </strong><strong>1</strong><strong>1</strong><strong>:
</strong><a href="https://www.davidjleifer.com/blogs/part11.html"><font color="#000080"><span lang="zxx"><u>https://www.davidjleifer.com/blogs/par</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part11.html"><font color="#000080"><span lang="zxx"><u>t1</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part11.html"><font color="#000080"><span lang="zxx"><u>1</u></span></font></a><a href="https://www.davidjleifer.com/blogs/part11.html"><font color="#000080"><span lang="zxx"><u>.html</u></span></font></a></p>
<p><b>Part 12: </b><a href="https://www.davidjleifer.com/blogs/part12.html"><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part12.html</span></a></p>
<p><b>Part 13: </b><a href="https://www.davidjleifer.com/blogs/part13.html"><span style="font-weight: normal">https://www.davidjleifer.com/blogs/part13.html</span></a></p>
</body>
</html>